{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 600_Cosine_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/danie/OneDrive/Desktop/openai_youtube_api_key.txt\") as f:\n",
    "    api_key = f.readline()\n",
    "\n",
    "oai_client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCLmEFIwfG3HSru_nUgOktGg</td>\n",
       "      <td>Triggered Newscast</td>\n",
       "      <td>R</td>\n",
       "      <td>__xDde9kpw0</td>\n",
       "      <td>Kevin McCarthy Nearly Defeats Nancy Pelosi in ...</td>\n",
       "      <td>#McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...</td>\n",
       "      <td>We the people need to start really fighting th...</td>\n",
       "      <td>UgzpCd27ArM_08w1XPh4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCLmEFIwfG3HSru_nUgOktGg</td>\n",
       "      <td>Triggered Newscast</td>\n",
       "      <td>R</td>\n",
       "      <td>__xDde9kpw0</td>\n",
       "      <td>Kevin McCarthy Nearly Defeats Nancy Pelosi in ...</td>\n",
       "      <td>#McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...</td>\n",
       "      <td>Bullshit. 2020 Election Fraud is off the hook....</td>\n",
       "      <td>Ugz4dCGe1U3YnWbXoDp4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCLmEFIwfG3HSru_nUgOktGg</td>\n",
       "      <td>Triggered Newscast</td>\n",
       "      <td>R</td>\n",
       "      <td>__xDde9kpw0</td>\n",
       "      <td>Kevin McCarthy Nearly Defeats Nancy Pelosi in ...</td>\n",
       "      <td>#McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...</td>\n",
       "      <td>How can the democrats commit political suicide...</td>\n",
       "      <td>Ugwlro6l4FK4FZ0PLoF4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCLmEFIwfG3HSru_nUgOktGg</td>\n",
       "      <td>Triggered Newscast</td>\n",
       "      <td>R</td>\n",
       "      <td>__xDde9kpw0</td>\n",
       "      <td>Kevin McCarthy Nearly Defeats Nancy Pelosi in ...</td>\n",
       "      <td>#McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...</td>\n",
       "      <td>I’m sure they made sure it was too close to co...</td>\n",
       "      <td>UgxARO3zib2Ve527Obd4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCLmEFIwfG3HSru_nUgOktGg</td>\n",
       "      <td>Triggered Newscast</td>\n",
       "      <td>R</td>\n",
       "      <td>__xDde9kpw0</td>\n",
       "      <td>Kevin McCarthy Nearly Defeats Nancy Pelosi in ...</td>\n",
       "      <td>#McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...</td>\n",
       "      <td>She’s a nasty mean drunk</td>\n",
       "      <td>UgzOZqdBWxXk-ixJQLV4AaABAg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id       channel_title affiliation     video_id  \\\n",
       "0  UCLmEFIwfG3HSru_nUgOktGg  Triggered Newscast           R  __xDde9kpw0   \n",
       "1  UCLmEFIwfG3HSru_nUgOktGg  Triggered Newscast           R  __xDde9kpw0   \n",
       "2  UCLmEFIwfG3HSru_nUgOktGg  Triggered Newscast           R  __xDde9kpw0   \n",
       "3  UCLmEFIwfG3HSru_nUgOktGg  Triggered Newscast           R  __xDde9kpw0   \n",
       "4  UCLmEFIwfG3HSru_nUgOktGg  Triggered Newscast           R  __xDde9kpw0   \n",
       "\n",
       "                                         video_title  \\\n",
       "0  Kevin McCarthy Nearly Defeats Nancy Pelosi in ...   \n",
       "1  Kevin McCarthy Nearly Defeats Nancy Pelosi in ...   \n",
       "2  Kevin McCarthy Nearly Defeats Nancy Pelosi in ...   \n",
       "3  Kevin McCarthy Nearly Defeats Nancy Pelosi in ...   \n",
       "4  Kevin McCarthy Nearly Defeats Nancy Pelosi in ...   \n",
       "\n",
       "                                   video_description  \\\n",
       "0  #McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...   \n",
       "1  #McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...   \n",
       "2  #McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...   \n",
       "3  #McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...   \n",
       "4  #McCarthy #KevinMcCarthy #DrainTheSwamp\\nDespi...   \n",
       "\n",
       "                                        comment_text  \\\n",
       "0  We the people need to start really fighting th...   \n",
       "1  Bullshit. 2020 Election Fraud is off the hook....   \n",
       "2  How can the democrats commit political suicide...   \n",
       "3  I’m sure they made sure it was too close to co...   \n",
       "4                           She’s a nasty mean drunk   \n",
       "\n",
       "                   comment_id  \n",
       "0  UgzpCd27ArM_08w1XPh4AaABAg  \n",
       "1  Ugz4dCGe1U3YnWbXoDp4AaABAg  \n",
       "2  Ugwlro6l4FK4FZ0PLoF4AaABAg  \n",
       "3  UgxARO3zib2Ve527Obd4AaABAg  \n",
       "4  UgzOZqdBWxXk-ixJQLV4AaABAg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = pd.read_csv('../data/cleaned/channel_subset_with_comments.csv')\n",
    "cleaned = cleaned[cleaned.channel_title.isin([\"TYT's The Conversation\", \"Turning Point USA\", 'TMM', 'Triggered Newscast'])].reset_index(drop=True)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Triggered Newscast', 'Turning Point USA', 'TMM',\n",
       "       \"TYT's The Conversation\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.channel_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "affils = ['progressive', 'conservative']\n",
    "model_sizes = [5**n for n in range(2, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get videos which have been used for training for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_titles = {}\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        used_titles[(affil, ms)] = []\n",
    "        file_name = f\"{affil}_size_{ms}_train.jsonl\" \n",
    "        with open(f'../data/final_ft_datasets/{file_name}', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                start_ind = line.rindex('You just watched the youtube video ') + len('You just watched the youtube video ') + 1\n",
    "                end_index = line.index(' with the description ') - 1\n",
    "                used_titles[(affil, ms)].append(line[start_ind:end_index])\n",
    "        used_titles[(affil, ms)] = list(set(used_titles[(affil, ms)]))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get new videos for models to comment on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_rows(group):\n",
    "    return group.sample(min(len(group), 250)) \n",
    "\n",
    "videos_to_comment_on = {}\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        affil_label = 'L' if affil == 'progressive' else 'R'\n",
    "        unused_titles = cleaned[~cleaned['video_title'].isin(used_titles[(affil, ms)])]\n",
    "        unused_titles = unused_titles[unused_titles.affiliation == affil_label]\n",
    "        unused_titles = unused_titles[unused_titles.channel_title.isin([\"TYT's The Conversation\", \"Turning Point USA\"])]\n",
    "        counts = unused_titles.video_title.value_counts()\n",
    "        # Filter the DataFrame to keep only rows where the column value occurs at least 10 times\n",
    "        filtered_df = unused_titles[unused_titles.video_title.isin(counts[counts >= 150].index)]\n",
    "        # Apply the function to each group within the specified column\n",
    "        filtered_df = filtered_df.groupby('video_title', group_keys=False).apply(sample_rows)\n",
    "        \n",
    "        unique_groups = filtered_df['video_title'].unique()\n",
    "\n",
    "        # If there are fewer than 4 unique groups, keep all of them\n",
    "        if len(unique_groups) <= 4:\n",
    "            groups_to_keep = unique_groups\n",
    "        else:\n",
    "            # Randomly select 4 groups to keep\n",
    "            np.random.shuffle(unique_groups)\n",
    "            groups_to_keep = unique_groups[:4]\n",
    "\n",
    "        # Filter the DataFrame to keep only rows from the selected groups\n",
    "        filtered_df = filtered_df[filtered_df['video_title'].isin(groups_to_keep)]\n",
    "        videos_to_comment_on[(affil, ms)] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_title\n",
      "INSANE Interviews with MAGA Supporters                250\n",
      "Should Medicare for All be Forced to a Floor Vote?    250\n",
      "Trump’s FINAL Play to Steal the Election              250\n",
      "Illegal Voter Purges in GEORGIA                       210\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election    250\n",
      "Illegal Voter Purges in GEORGIA             210\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election    250\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election    250\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Sex And Gender Are Directly Related                                                               211\n",
      "Candace Owens CRUSHES Modern Feminism | Why Feminists Demonize Men Instead Of Empowering Women    203\n",
      "We’ve Had Enough!! [Real America Episode 29]                                                      201\n",
      "The Hodge Twins On Being Called \"Sellouts\" | Always Stand Up For Your Beliefs                     164\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Candace Owens Triggers Crazy Lib | Why Can't Leftists Handle Differing Opinions?    250\n",
      "ANTIFA SHOWN LAW AND ORDER...                                                       217\n",
      "Rand Paul Exposes The Truth About COVID Hysteria | Do Lockdowns Work?               211\n",
      "LEFTIST GROUPS RAGE IN PARIS...                                                     157\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk And Candace Owens Debunk \"White Privilege\"  | Angry Leftist Gets OWNED              250\n",
      "We’ve Had Enough!! [Real America Episode 29]                                                     201\n",
      "Candace Owens Debunks \"White Privilege\" | Leftist Rhetoric Doesn't Stop Racism, It Spreads It    193\n",
      "The Hodge Twins On Being Called \"Sellouts\" | Always Stand Up For Your Beliefs                    164\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk Deconstructs The Left's Race Obsession | Why \"White Privilege\" Only Upholds Racism    250\n",
      "We’ve Had Enough!! [Real America Episode 29]                                                       201\n",
      "Candace Owens Debunks \"White Privilege\" | Leftist Rhetoric Doesn't Stop Racism, It Spreads It      193\n",
      "The Hodge Twins On Being Called \"Sellouts\" | Always Stand Up For Your Beliefs                      164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for model in videos_to_comment_on:\n",
    "    print(videos_to_comment_on[model].video_title.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a dictionary for each model to comment on a video from the other channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('progressive', 25): {'video_title': 'Yet Another 10 Questions for Atheists',\n",
       "  'video_description': 'The whole video:\\nYes, I do have a Patreon account, thank you for asking:\\nMy Twitter:\\nMy facebook:\\nHere’s my society6 store if you’re interested in my pretentious minimalist poster designs:\\n'},\n",
       " ('progressive', 125): {'video_title': 'Was Jesus Really Buried?',\n",
       "  'video_description': \"The whole video:\\nNonstampcollector's video:\\nYes, I do have a Patreon account, thank you for asking:\\nMy Twitter:\\nMy facebook:\\nHere’s my society6 store if you’re interested in my pretentious minimalist poster designs:\\n\"},\n",
       " ('progressive', 625): {'video_title': \"Why the Universe Doesn't Need a Cause\",\n",
       "  'video_description': 'The whole video:\\nYes, I do have a Patreon account, thank you for asking:\\nMy Twitter:\\nMy facebook:\\nHere’s my society6 store if you’re interested in my pretentious minimalist poster designs:\\n'},\n",
       " ('progressive',\n",
       "  3125): {'video_title': 'Yet Another 10 Questions for Atheists', 'video_description': 'The whole video:\\nYes, I do have a Patreon account, thank you for asking:\\nMy Twitter:\\nMy facebook:\\nHere’s my society6 store if you’re interested in my pretentious minimalist poster designs:\\n'},\n",
       " ('conservative',\n",
       "  25): {'video_title': 'PENCECARD Discarded - Trump Has Different Plan', 'video_description': \"While the hashtag #PenceCard was trending all day on twitter, as it was highly popularized by the National File, it appears that is is not the Plan of President Trump or his council to invoke the Pence Card.\\nApparently the President is relying on the challenges to the state electors, and voting irregularities that he and his council have alleged against the democratic party, by use of illicit software and malicious efforts to remove GOP poll watchers and delegates.\\nDespite a significant amount of Trump voters (up to 70%) and Biden voters (as many as 30%) have signaled their dissatisfaction, and distrust of the 2020 Presidential Election. While many trump supporters believed that this Pence card could be advantageous when compared to waiting on SCOTUS decisions, or the possibility of utilizing the insurrection act.\\nSources:\\nNational File Article\\nSnopes Claims It False:\\nTrump vs PA in SCOTUS\\nLet us know what you think below.\\nPlease give this video a thumbs up and share it with a friend or family member who is a likeminded Patriot who would love this content.\\nConsider subscribing and ringing the bell for notifications like you're punching Nancy Pelosi in the head!\\nThis is the Triggered Newscast.\\nWe'll see you at the next report.\\n\"},\n",
       " ('conservative',\n",
       "  125): {'video_title': \"Woman Yells Nancy Pelosi's Quotes at Chuck Schumer\", 'video_description': \"Woman Yells Nancy Pelosi's Quotes At Chuck Schumer and shuts down the press conference. In the full video clip you could hear Schumer ask agents if they could remove her; however, it appears that her quoting Nancy #PelosiQuotes was not enough to have her removed from the scene.\\n#Pelosi instigated left leaning protestors, and this woman calls out the hypocrisy of blaming Trump for the actions of his Supporters, rather than for the actions of the democrats and rhinos not willing to do what is best for we the people.\\nShe also accuses democrat governors, mayors, congressmembers and senators like #ChuckSchumer, of asking for a Police to Stand down, while using inflammatory language.\\nLet us know what you think down below!\\nWe'll see you at the next Triggered Newscast.\\nSource:\\n\"},\n",
       " ('conservative',\n",
       "  625): {'video_title': 'ELECTORAL COLLEGE SESSION COUNT AND DEBATE SHUT DOWN BY AMERICANS FLOODING THE CAPITOL', 'video_description': 'WOW GUYS! HUGE NEWS, the debate at the Capitol has been shut down, as the congress and senate chambers have been entered by protestors.\\nThe entire session is locked down on recess right now and will likely be held at an undisclosed location.\\n#CapitolHill #TRUMP #JointSessionofCongress\\n'},\n",
       " ('conservative',\n",
       "  3125): {'video_title': 'Senators Betray The Republic, Donald Trump, and We The People', 'video_description': 'Senators decide to certify Biden and go back on bid for electoral commission, by blaming the events of this morning as justification for no longer needed to debate if the elections in these states followed the constitution.\\nBy choosing not to debate they have done far more of a disservice to our great country.\\nLet me know what you think, down below.\\n'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_to_comment_on_other_channels = {}\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        videos_to_comment_on_other_channels[(affil, ms)] = {}\n",
    "        affil_label = 'L' if affil == 'progressive' else 'R'\n",
    "        possible_videos = cleaned[~cleaned.channel_title.isin([\"TYT's The Conversation\", \"Turning Point USA\"])]\n",
    "        possible_videos = possible_videos[possible_videos.affiliation == affil_label]\n",
    "        title = random.sample(possible_videos.video_title.tolist(), k=1)[0]\n",
    "        videos_to_comment_on_other_channels[(affil, ms)]['video_title'] = title\n",
    "        videos_to_comment_on_other_channels[(affil, ms)]['video_description'] = possible_videos[possible_videos.video_title == title].video_description.unique()[0]\n",
    "\n",
    "videos_to_comment_on_other_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's get the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_for_chat_completion(title, description, affil):\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {affil} American political commentator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"You just watched the youtube video '{title}' with the description '{description}'.\\n\\nGive your opinion on the subject matter.\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliated_models_jobs = {\n",
    "    'l' : {\n",
    "        25: \"ftjob-0y1Pfg6iOvUaWyZzYuRq4TJI\",\n",
    "        125: \"ftjob-GBtoSSh1102sIQbZT954WNR0\",\n",
    "        625: \"ftjob-HMEEAz2r4LVmtdFIo5zHRRWC\",\n",
    "        3125: \"ftjob-Pop4MECv0nHNdBcYZYy0rSjb\",\n",
    "        15625: \"ftjob-AEqdzMVWiZhMxv8Y6m3b1pHE\"\n",
    "    },\n",
    "    \n",
    "    'r' : {\n",
    "        25: \"ftjob-q5UHn926xz9g8wfdCPjFuK7U\",\n",
    "        125: \"ftjob-dUQD3xBBWUQ33mA1AcTRUxSr\",\n",
    "        625: \"ftjob-p9GPwKfqlV0Zs32kjwkay703\",\n",
    "        3125: \"ftjob-gSeeH5p5VPXMA2AwyKqGs11S\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': {25: 'ft:gpt-3.5-turbo-0613:personal::8HuIhUqH',\n",
       "  125: 'ft:gpt-3.5-turbo-0613:personal::8Hu2dTTZ',\n",
       "  625: 'ft:gpt-3.5-turbo-0613:personal::8HteyfJc',\n",
       "  3125: 'ft:gpt-3.5-turbo-0613:personal::8HsYApwC',\n",
       "  15625: 'ft:gpt-3.5-turbo-0613:personal::8HizTQw1'},\n",
       " 'r': {25: 'ft:gpt-3.5-turbo-0613:personal::8HzsJtSa',\n",
       "  125: 'ft:gpt-3.5-turbo-0613:personal::8Hzc3hWd',\n",
       "  625: 'ft:gpt-3.5-turbo-0613:personal::8HxCZnqf',\n",
       "  3125: 'ft:gpt-3.5-turbo-0613:personal::8HviUlwy'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affiliated_models = {\n",
    "    affil : {   k: oai_client.fine_tuning.jobs.retrieve(v).fine_tuned_model for (k,v) in model_dic.items()  }\n",
    "        for affil, model_dic in affiliated_models_jobs.items()\n",
    "}\n",
    "\n",
    "affiliated_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_comments_and_responses = {}\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        affil_label = 'l' if affil == 'progressive' else 'r'\n",
    "        videos_comments_and_responses[(affil, ms)] = []\n",
    "        rows = videos_to_comment_on[(affil, ms)]\n",
    "        for video in rows.video_title.unique():\n",
    "            video_dic = {}\n",
    "            video_dic['video_title'] = video\n",
    "            video_rows = rows[rows.video_title == video]\n",
    "            video_dic['human_comments'] = video_rows.comment_text.tolist()\n",
    "            message_list = format_question_for_chat_completion(video_rows.video_title, video_rows.video_description, affil)\n",
    "            n = video_rows.shape[0] \n",
    "            if n > 128:\n",
    "                second_run = n-128\n",
    "                response1 = oai_client.chat.completions.create(\n",
    "                    model=affiliated_models[affil_label][ms],\n",
    "                    messages=message_list,\n",
    "                    n=128\n",
    "                )\n",
    "                temp1 = [choice.message.content for choice in response1.choices]\n",
    "                response2 = oai_client.chat.completions.create(\n",
    "                    model=affiliated_models[affil_label][ms],\n",
    "                    messages=message_list,\n",
    "                    n=second_run\n",
    "                )\n",
    "                temp2 = [choice.message.content for choice in response2.choices]\n",
    "                video_dic['llm_comments'] = temp1+temp2\n",
    "            else:\n",
    "                response = oai_client.chat.completions.create(\n",
    "                    model=affiliated_models[affil_label][ms],\n",
    "                    messages=message_list,\n",
    "                    n=n\n",
    "                )\n",
    "                video_dic['llm_comments'] = [choice.message.content for choice in response.choices]\n",
    "            videos_comments_and_responses[(affil, ms)].append(video_dic)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        affil_label = 'l' if affil == 'progressive' else 'r'\n",
    "        info = videos_to_comment_on_other_channels[(affil, ms)]\n",
    "        message_list = format_question_for_chat_completion(info['video_title'], video_rows['video_description'], affil)\n",
    "        response1 = oai_client.chat.completions.create(\n",
    "            model=affiliated_models[affil_label][ms],\n",
    "            messages=message_list,\n",
    "            n=125\n",
    "        )\n",
    "        temp1 = [choice.message.content for choice in response1.choices]\n",
    "        response2 = oai_client.chat.completions.create(\n",
    "            model=affiliated_models[affil_label][ms],\n",
    "            messages=message_list,\n",
    "            n=125\n",
    "        )\n",
    "        temp2 = [choice.message.content for choice in response2.choices]\n",
    "        videos_to_comment_on_other_channels[(affil, ms)]['llm_comments'] = temp1+temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_dic = {}\n",
    "for affil in affils:\n",
    "    output_dic[affil] = {}\n",
    "    for ms in model_sizes:\n",
    "        output_dic[affil][ms] = videos_comments_and_responses[(affil, ms)]\n",
    "with open(\"../data/experiment_rerun/consine_similarity_responses.json\", 'w') as file:\n",
    "    json.dump(output_dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_comments_and_responses_other_channels = videos_to_comment_on_other_channels\n",
    "\n",
    "output_dic = {}\n",
    "for affil in affils:\n",
    "    output_dic[affil] = {}\n",
    "    for ms in model_sizes:\n",
    "        output_dic[affil][ms] = videos_comments_and_responses_other_channels[(affil, ms)]\n",
    "        \n",
    "\n",
    "with open(\"../data/experiment_rerun/consine_similarity_responses_other_channels.json\", 'w') as file:\n",
    "    json.dump(output_dic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "videos_comments_and_responses_embedded = deepcopy(videos_comments_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"): # model = \"deployment_name\"\n",
    "    return oai_client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Model: progressive,25\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: progressive,125\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: progressive,625\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: progressive,3125\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: conservative,25\n",
      "0 comments skipped, 1 comments skipped\n",
      "For Model: conservative,125\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: conservative,625\n",
      "0 comments skipped, 0 comments skipped\n",
      "For Model: conservative,3125\n",
      "0 comments skipped, 0 comments skipped\n"
     ]
    }
   ],
   "source": [
    "from openai import BadRequestError\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        skipped_human = 0\n",
    "        skipped_llm = 0\n",
    "        for i in range(len(videos_comments_and_responses_embedded[(affil, ms)])):\n",
    "            human_arr = []\n",
    "            for x in videos_comments_and_responses_embedded[(affil, ms)][i]['human_comments']:\n",
    "                try:\n",
    "                    human_arr.append(get_embedding(x))\n",
    "                except BadRequestError as e:\n",
    "                    skipped_human += 1\n",
    "                    continue\n",
    "            videos_comments_and_responses_embedded[(affil, ms)][i]['human_comments'] = human_arr\n",
    "            \n",
    "            llm_arr = []\n",
    "            for x in videos_comments_and_responses_embedded[(affil, ms)][i]['llm_comments']:\n",
    "                try:\n",
    "                    llm_arr.append(get_embedding(x))\n",
    "                except BadRequestError as e:\n",
    "                    skipped_llm += 1      \n",
    "                    continue\n",
    "            videos_comments_and_responses_embedded[(affil, ms)][i]['llm_comments'] = llm_arr\n",
    "        print(f\"For Model: {affil},{ms}\")\n",
    "        print(f\"{skipped_human} comments skipped, {skipped_llm} comments skipped\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dic = {}\n",
    "for affil in affils:\n",
    "    output_dic[affil] = {}\n",
    "    for ms in model_sizes:\n",
    "        output_dic[affil][ms] = videos_comments_and_responses_embedded[(affil, ms)]\n",
    "with open(\"../data/experiment_rerun/consine_similarity_embedded.json\", 'w') as file:\n",
    "    json.dump(output_dic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It broke. Lets load the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/experiment_rerun/consine_similarity_embedded.json\", 'r') as file:\n",
    "    videos_comments_and_responses_embedded= json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/experiment_rerun/consine_similarity_responses_other_channels.json\", 'r') as file:\n",
    "    videos_comments_and_responses_other_channels_embedded = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Model: progressive,25\n",
      "1 comments skipped\n",
      "For Model: progressive,125\n",
      "0 comments skipped\n",
      "For Model: progressive,625\n",
      "0 comments skipped\n",
      "For Model: progressive,3125\n",
      "0 comments skipped\n",
      "For Model: conservative,25\n",
      "0 comments skipped\n",
      "For Model: conservative,125\n",
      "0 comments skipped\n",
      "For Model: conservative,625\n",
      "0 comments skipped\n",
      "For Model: conservative,3125\n",
      "0 comments skipped\n"
     ]
    }
   ],
   "source": [
    "from openai import BadRequestError\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        skipped_llm = 0\n",
    "        llm_arr = []\n",
    "        for x in videos_comments_and_responses_other_channels_embedded[affil][str(ms)]['llm_comments']:\n",
    "            try:\n",
    "                llm_arr.append(get_embedding(x))\n",
    "            except BadRequestError as e:\n",
    "                skipped_llm += 1      \n",
    "                continue\n",
    "        videos_comments_and_responses_other_channels_embedded[affil][str(ms)]['llm_comments'] = llm_arr\n",
    "        print(f\"For Model: {affil},{ms}\")\n",
    "        print(f\"{skipped_llm} comments skipped\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/experiment_rerun/consine_similarity_other_channels_embedded.json\", 'w') as file:\n",
    "    json.dump(videos_comments_and_responses_other_channels_embedded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/experiment_rerun/consine_similarity_other_channels_embedded.json\", 'r') as file:\n",
    "    videos_comments_and_responses_other_channels_embedded = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation</th>\n",
       "      <th>size</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>similarity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [affiliation, size, trial_id, similarity, similarity_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_part1_df = pd.DataFrame(columns = ['affiliation',\n",
    "                                      'size',\n",
    "                                      'trial_id',\n",
    "                                      'similarity',\n",
    "                                      'similarity_type'])\n",
    "cs_part1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_12228\\3215319555.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cs_part1_df = pd.concat([cs_part1_df, df])\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        data = {\n",
    "                'trial_id' : [],\n",
    "                'similarity' : [],\n",
    "                'similarity_type' : []   \n",
    "            }\n",
    "        for trial_id in range(1, 1001):\n",
    "            \n",
    "            selected_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            human_comment_for_comparison = sample(videos_comments_and_responses_embedded[affil][str(ms)][selected_video]['human_comments'], k=1)\n",
    "            human_comment_list = sample([x for x in videos_comments_and_responses_embedded[affil][str(ms)][selected_video]['human_comments'] if x != human_comment_for_comparison], 100)\n",
    "            llm_comment_list = sample(videos_comments_and_responses_embedded[affil][str(ms)][selected_video]['llm_comments'], 100)\n",
    "            \n",
    "            data['trial_id'].append(trial_id)\n",
    "            data['similarity'].append(np.mean([cosine_similarity(human_comment_for_comparison, x) for x in human_comment_list]))\n",
    "            data['similarity_type'].append('human')\n",
    "            data['trial_id'].append(trial_id)\n",
    "            data['similarity'].append(np.mean([cosine_similarity(human_comment_for_comparison, x) for x in llm_comment_list]))\n",
    "            data['similarity_type'].append('synthetic')\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        df['affiliation'] = affil\n",
    "        df['size'] = ms\n",
    "        cs_part1_df = pd.concat([cs_part1_df, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation</th>\n",
       "      <th>size</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>similarity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>progressive</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.182299</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>progressive</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.174904</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.173855</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.182361</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.143762</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>conservative</td>\n",
       "      <td>3125</td>\n",
       "      <td>998</td>\n",
       "      <td>0.109620</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>conservative</td>\n",
       "      <td>3125</td>\n",
       "      <td>999</td>\n",
       "      <td>0.275310</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>conservative</td>\n",
       "      <td>3125</td>\n",
       "      <td>999</td>\n",
       "      <td>0.190809</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>conservative</td>\n",
       "      <td>3125</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.179381</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>conservative</td>\n",
       "      <td>3125</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.155005</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       affiliation  size trial_id  similarity similarity_type\n",
       "0      progressive    25        1    0.182299           human\n",
       "1      progressive    25        1    0.174904       synthetic\n",
       "2      progressive    25        2    0.173855           human\n",
       "3      progressive    25        2    0.182361       synthetic\n",
       "4      progressive    25        3    0.143762           human\n",
       "...            ...   ...      ...         ...             ...\n",
       "1995  conservative  3125      998    0.109620       synthetic\n",
       "1996  conservative  3125      999    0.275310           human\n",
       "1997  conservative  3125      999    0.190809       synthetic\n",
       "1998  conservative  3125     1000    0.179381           human\n",
       "1999  conservative  3125     1000    0.155005       synthetic\n",
       "\n",
       "[16000 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_part1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_part1_df.to_csv(\"../data/experiment_rerun/cosine_similarity_human_synthetic_comparisons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             similarity   R-squared:                       0.061\n",
      "Model:                            OLS   Adj. R-squared:                  0.061\n",
      "Method:                 Least Squares   F-statistic:                     1040.\n",
      "Date:                Wed, 06 Mar 2024   Prob (F-statistic):          4.04e-221\n",
      "Time:                        09:39:39   Log-Likelihood:                 27487.\n",
      "No. Observations:               16000   AIC:                        -5.497e+04\n",
      "Df Residuals:                   15998   BIC:                        -5.496e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================================\n",
      "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Intercept                           0.2111      0.000    434.777      0.000       0.210       0.212\n",
      "C(similarity_type)[T.synthetic]    -0.0221      0.001    -32.249      0.000      -0.023      -0.021\n",
      "==============================================================================\n",
      "Omnibus:                       58.954   Durbin-Watson:                   1.141\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               44.664\n",
      "Skew:                           0.000   Prob(JB):                     2.00e-10\n",
      "Kurtosis:                       2.741   Cond. No.                         2.62\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# NB. unlike sm.OLS, there is \"intercept\" term is included here\n",
    "lm = smf.ols(formula=\"similarity ~ C(similarity_type)\", data=cs_part1_df).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation</th>\n",
       "      <th>size</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>similarity_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [affiliation, size, trial_id, similarity, similarity_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_part2_df = pd.DataFrame(columns = ['affiliation',\n",
    "                                      'size',\n",
    "                                      'trial_id',\n",
    "                                      'similarity',\n",
    "                                      'similarity_type'\n",
    "                                      ])\n",
    "cs_part2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_rows(group):\n",
    "    return group.sample(min(len(group), 250)) \n",
    "\n",
    "videos_to_comment_on_2 = {}\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        affil_label = 'L' if affil == 'progressive' else 'R'\n",
    "        unused_titles = cleaned[~cleaned['video_title'].isin(used_titles[(affil, ms)])]\n",
    "        unused_titles = unused_titles[unused_titles.affiliation == affil_label]\n",
    "        filtered_df = unused_titles[unused_titles.channel_title.isin([\"TYT's The Conversation\", \"Turning Point USA\"])]\n",
    "        '''\n",
    "        counts = unused_titles.video_title.value_counts()\n",
    "        # Filter the DataFrame to keep only rows where the column value occurs at least 10 times\n",
    "        filtered_df = unused_titles[unused_titles.video_title.isin(counts[counts >= 5].index)]\n",
    "        # Apply the function to each group within the specified column\n",
    "        filtered_df = filtered_df.groupby('video_title', group_keys=False).apply(sample_rows)\n",
    "        \n",
    "        unique_groups = filtered_df['video_title'].unique()\n",
    "\n",
    "        # If there are fewer than 4 unique groups, keep all of them\n",
    "        if len(unique_groups) <= 4:\n",
    "            groups_to_keep = unique_groups\n",
    "        else:\n",
    "            # Randomly select 4 groups to keep\n",
    "            np.random.shuffle(unique_groups)\n",
    "            groups_to_keep = unique_groups[:4]\n",
    "\n",
    "        # Filter the DataFrame to keep only rows from the selected groups\n",
    "        filtered_df = filtered_df[filtered_df['video_title'].isin(groups_to_keep)]'''\n",
    "        videos_to_comment_on_2[(affil, ms)] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_title\n",
      "Should Medicare for All be Forced to a Floor Vote?    506\n",
      "INSANE Interviews with MAGA Supporters                475\n",
      "Trump’s FINAL Play to Steal the Election              288\n",
      "Illegal Voter Purges in GEORGIA                       210\n",
      "Who Is Behind QANON?                                  128\n",
      "                                                     ... \n",
      "Drag Queen Wants to SAVE the Environment                6\n",
      "Banks Continue To FAIL Minority Communities             5\n",
      "The Threat of Right-wing Terrorists                     4\n",
      "Georgia is the Newest BATTLEGROUND State                3\n",
      "The Long Battle For Police Accountability               2\n",
      "Name: count, Length: 62, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election                        288\n",
      "Illegal Voter Purges in GEORGIA                                 210\n",
      "Who Is Behind QANON?                                            128\n",
      "Racial JUSTICE Under the Biden Administration                   118\n",
      "First Progressive News Show for SPANISH-Speaking Audience        70\n",
      "The End of Trump's DEATH Cult?                                   64\n",
      "Teens LEAD Voter Movement in Georgia                             62\n",
      "Why Democrats Need to Stop Compromising and FIGHT                59\n",
      "Trump Enablers Can Be REMOVED From Office                        52\n",
      "Watch This POWERFUL Georgia Election Ad                          48\n",
      "Trump's ATTACK on Freedom of the Press                           46\n",
      "McConnell BLOCKS $2000 Stimulus Checks                           46\n",
      "Most DANGEROUS Fox News Moment                                   40\n",
      "CALLING OUT Obama's Defund the Police Comments                   38\n",
      "What's at Stake with the Georgia Runoffs                         36\n",
      "The Police Killing of Casey Goodson                              33\n",
      "Will Biden PRESSURE Republicans?                                 28\n",
      "The Effect of Covid-19 On Climate Change                         27\n",
      "Trump's Pardoning Free-for-all                                   26\n",
      "Police HYPOCRISY During the Capitol Attacks                      25\n",
      "How Progressive Policies Are POSSIBLE                            24\n",
      "Billboards Push Trump Supporters to SIT OUT Runoff Elections     23\n",
      "Can Biden Really UNITE America?                                  22\n",
      "Justice Democrats Are RECRUITING                                 22\n",
      "What a Blue Georgia Means For Progressives                       21\n",
      "Wall Street Wants to Trade WATER                                 21\n",
      "How to Use ARTIVISM to Support Black Lives                       20\n",
      "MAGA's FAILED Coup Attempt                                       19\n",
      "College Students Are on a Tuition STRIKE                         19\n",
      "VIRAL Activist Running for Congress                              19\n",
      "Ken Klippenstein's FOIA Lawyer on Their Latest Successes         17\n",
      "Bringing Compassion to Prisons                                   17\n",
      "How Georgia Flipped BLUE                                         15\n",
      "Biden Appointee Has Ties to Betsy DeVos                          15\n",
      "Increased RISKS for Female Restaurant Workers During Covid       14\n",
      "The NFL's Cheerleading Problem                                   13\n",
      "No HONEYMOON For Biden                                           12\n",
      "Comedy’s Role in Our POLITICAL Landscape                         11\n",
      "Is Biden Making Climate-Friendly Cabinet Picks?                  11\n",
      "Environmental RACISM in Los Angeles                              11\n",
      "This Candidate Will FIGHT for Working Class People               11\n",
      "How Nativism FUELS the Radical Right                             11\n",
      "Disrupting Racism in Tech Startups                               10\n",
      "Rolling Back Trump's Discriminatory Policies                      9\n",
      "Affordable Housing During Covid-19                                9\n",
      "Surprise Billing During Covid-19                                  9\n",
      "Wrongful Convictions RAMPANT In the U.S.                          8\n",
      "BIPARTISAN Win for Alaska's Wildlife                              8\n",
      "Misconceptions Around Poverty in America                          8\n",
      "Daily Show Creator on Her New Comedy Special                      7\n",
      "Progressives Can REVERSE Inequality                               6\n",
      "Drag Queen Wants to SAVE the Environment                          6\n",
      "Banks Continue To FAIL Minority Communities                       5\n",
      "The Threat of Right-wing Terrorists                               4\n",
      "Georgia is the Newest BATTLEGROUND State                          3\n",
      "The Long Battle For Police Accountability                         2\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election                        288\n",
      "Why Democrats Need to Stop Compromising and FIGHT                59\n",
      "McConnell BLOCKS $2000 Stimulus Checks                           46\n",
      "Most DANGEROUS Fox News Moment                                   40\n",
      "CALLING OUT Obama's Defund the Police Comments                   38\n",
      "Trump's Pardoning Free-for-all                                   26\n",
      "Police HYPOCRISY During the Capitol Attacks                      25\n",
      "How Progressive Policies Are POSSIBLE                            24\n",
      "Billboards Push Trump Supporters to SIT OUT Runoff Elections     23\n",
      "Justice Democrats Are RECRUITING                                 22\n",
      "Can Biden Really UNITE America?                                  22\n",
      "How to Use ARTIVISM to Support Black Lives                       20\n",
      "MAGA's FAILED Coup Attempt                                       19\n",
      "College Students Are on a Tuition STRIKE                         19\n",
      "VIRAL Activist Running for Congress                              19\n",
      "Ken Klippenstein's FOIA Lawyer on Their Latest Successes         17\n",
      "Bringing Compassion to Prisons                                   17\n",
      "Biden Appointee Has Ties to Betsy DeVos                          15\n",
      "How Georgia Flipped BLUE                                         15\n",
      "Increased RISKS for Female Restaurant Workers During Covid       14\n",
      "No HONEYMOON For Biden                                           12\n",
      "Comedy’s Role in Our POLITICAL Landscape                         11\n",
      "Environmental RACISM in Los Angeles                              11\n",
      "This Candidate Will FIGHT for Working Class People               11\n",
      "Is Biden Making Climate-Friendly Cabinet Picks?                  11\n",
      "Disrupting Racism in Tech Startups                               10\n",
      "Surprise Billing During Covid-19                                  9\n",
      "Affordable Housing During Covid-19                                9\n",
      "Rolling Back Trump's Discriminatory Policies                      9\n",
      "Wrongful Convictions RAMPANT In the U.S.                          8\n",
      "BIPARTISAN Win for Alaska's Wildlife                              8\n",
      "Daily Show Creator on Her New Comedy Special                      7\n",
      "Progressives Can REVERSE Inequality                               6\n",
      "Drag Queen Wants to SAVE the Environment                          6\n",
      "Banks Continue To FAIL Minority Communities                       5\n",
      "The Threat of Right-wing Terrorists                               4\n",
      "Georgia is the Newest BATTLEGROUND State                          3\n",
      "The Long Battle For Police Accountability                         2\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Trump’s FINAL Play to Steal the Election                        288\n",
      "How Progressive Policies Are POSSIBLE                            24\n",
      "Billboards Push Trump Supporters to SIT OUT Runoff Elections     23\n",
      "Can Biden Really UNITE America?                                  22\n",
      "How to Use ARTIVISM to Support Black Lives                       20\n",
      "College Students Are on a Tuition STRIKE                         19\n",
      "Ken Klippenstein's FOIA Lawyer on Their Latest Successes         17\n",
      "Biden Appointee Has Ties to Betsy DeVos                          15\n",
      "This Candidate Will FIGHT for Working Class People               11\n",
      "Comedy’s Role in Our POLITICAL Landscape                         11\n",
      "Disrupting Racism in Tech Startups                               10\n",
      "Affordable Housing During Covid-19                                9\n",
      "Drag Queen Wants to SAVE the Environment                          6\n",
      "Progressives Can REVERSE Inequality                               6\n",
      "The Threat of Right-wing Terrorists                               4\n",
      "Georgia is the Newest BATTLEGROUND State                          3\n",
      "The Long Battle For Police Accountability                         2\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk And Candace Owens Debunk \"White Privilege\"  | Angry Leftist Gets OWNED                1751\n",
      "Charlie Kirk Deconstructs The Left's Race Obsession | Why \"White Privilege\" Only Upholds Racism     652\n",
      "WOW: TEACHER GOES INSANE ON PROTESTERS...                                                           452\n",
      "My Fellow Americans.. [Real America Episode 32]                                                     446\n",
      "Dear Mr. President... [Real America Episode 34]                                                     402\n",
      "                                                                                                   ... \n",
      "LIVE! SAS 2020 Day 3! Alex Marlow, Dave Rubin, Charlie Kirk and More!                                14\n",
      "Will Witt Gives AWESOME Advice To Young Student Activists At TPUSA's Student Action Summit 2020      11\n",
      "Sara Carter: You Are On The Frontlines Of A Battle That Is Not Ending                                10\n",
      "Dr. Jeff Myers: \"Nobody Is Neutral. Everyone Has A Worldview.\"                                        9\n",
      "Varsity Spirit: Only in America                                                                       3\n",
      "Name: count, Length: 118, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk And Candace Owens Debunk \"White Privilege\"  | Angry Leftist Gets OWNED                     1751\n",
      "Charlie Kirk Deconstructs The Left's Race Obsession | Why \"White Privilege\" Only Upholds Racism          652\n",
      "Candace Owens POWERFUL White House Speech | A Historical Moment At The Young Black Leadership Summit     320\n",
      "Candace Owens Triggers Crazy Lib | Why Can't Leftists Handle Differing Opinions?                         285\n",
      "Candace Owens Shatters Victim Mentality | Pretending To Be Oppressed Isn't Cool                          238\n",
      "                                                                                                        ... \n",
      "LIVE! SAS 2020 Day 3! Alex Marlow, Dave Rubin, Charlie Kirk and More!                                     14\n",
      "Will Witt Gives AWESOME Advice To Young Student Activists At TPUSA's Student Action Summit 2020           11\n",
      "Sara Carter: You Are On The Frontlines Of A Battle That Is Not Ending                                     10\n",
      "Dr. Jeff Myers: \"Nobody Is Neutral. Everyone Has A Worldview.\"                                             9\n",
      "Varsity Spirit: Only in America                                                                            3\n",
      "Name: count, Length: 89, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk And Candace Owens Debunk \"White Privilege\"  | Angry Leftist Gets OWNED                    1751\n",
      "Charlie Kirk Deconstructs The Left's Race Obsession | Why \"White Privilege\" Only Upholds Racism         652\n",
      "We’ve Had Enough!! [Real America Episode 29]                                                            201\n",
      "Candace Owens Debunks \"White Privilege\" | Leftist Rhetoric Doesn't Stop Racism, It Spreads It           193\n",
      "The Hodge Twins On Being Called \"Sellouts\" | Always Stand Up For Your Beliefs                           164\n",
      "What Happened To Unity? [Real America Episode 33]                                                       121\n",
      "TUCKER: KIDS NEED TO GO BACK TO SCHOOL...                                                               109\n",
      "Dinesh D'Souza: Conservatives Need To Refuse To Live In The Left's America                              101\n",
      "RONALD REAGAN'S THANKSGIVING MESSAGE...                                                                  98\n",
      "Charlie Kirk Answers The Question \"Why Do You Have To Be So Offensive?\" | Why Only Facts Matter          88\n",
      "Teachers Unions Harm Black American Youth                                                                76\n",
      "Leftist Santas?! [Real America Episode 28]                                                               75\n",
      "Rob Smith: \"Socialism Is About Rules For Thee, Not For Me\"                                               73\n",
      "Rep. Gaetz To TPUSA Activists: \"Every Job In America Is Essential\" TPUSA Student Action Summit 2020      71\n",
      "TUCKER SHREDS LEFT-WING “DIVERSITY” GAMES...                                                             60\n",
      "Sen. Rand Paul: One Of America's Greatest Freedom Fighters At TPUSA's Student Action Summit 2020         58\n",
      "Ask Yourself This Question Before Going To College                                                       54\n",
      "Leftist Vs. Liberal | Dave Rubin On How The Left Acts Like Thanos                                        53\n",
      "Kristi Noem Gives the BEST Advice | How to Share Opinions with Others                                    49\n",
      "Charlie Kirk Obliterates Tyrannical Lockdowns | Can America Return To Normal?                            44\n",
      "James O'Keefe: One Of America's Greatest Warriors For Truth At TPUSA's Student Action Summit 2020        38\n",
      "Mike Lindell's AMAZING Story Inspires Thousands Of Patriots At TPUSA's Student Action Summit 2020        36\n",
      "Graham Allen Tears Apart The Left & Oppressive Big Government At TPUSA's Student Action Summit 2020      35\n",
      "David Harris Jr. Speaks The TRUTH At TPUSA's Student Action Summit 2020!                                 34\n",
      "Charlie Kirk Has Built An OUTSTANDING Movement Of Freedom-Fighting Activists                             33\n",
      "Jordan Peterson: The Fundamental Necessity of Hierarchy | Full Speech                                    33\n",
      "Kristi Noem: Bad Government Is Why I'm Governor Today                                                    32\n",
      "What Are Your New Year’s Resolutions? [Benny On The Block 34]                                            30\n",
      "TPUSA's 2020 Year In Review                                                                              21\n",
      "Merry Christmas! [Real America Episode 30]                                                               19\n",
      "Best Of Real America! [Real America Episode 31]                                                          18\n",
      "Dan Bongino Slams Government Spending | Why Socialism NEVER Works                                        18\n",
      "LIVE! SAS 2020 Day 3! Alex Marlow, Dave Rubin, Charlie Kirk and More!                                    14\n",
      "Will Witt Gives AWESOME Advice To Young Student Activists At TPUSA's Student Action Summit 2020          11\n",
      "Dr. Jeff Myers: \"Nobody Is Neutral. Everyone Has A Worldview.\"                                            9\n",
      "Varsity Spirit: Only in America                                                                           3\n",
      "Name: count, dtype: int64\n",
      "video_title\n",
      "Charlie Kirk And Candace Owens Debunk \"White Privilege\"  | Angry Leftist Gets OWNED                    1751\n",
      "Charlie Kirk Deconstructs The Left's Race Obsession | Why \"White Privilege\" Only Upholds Racism         652\n",
      "We’ve Had Enough!! [Real America Episode 29]                                                            201\n",
      "Candace Owens Debunks \"White Privilege\" | Leftist Rhetoric Doesn't Stop Racism, It Spreads It           193\n",
      "The Hodge Twins On Being Called \"Sellouts\" | Always Stand Up For Your Beliefs                           164\n",
      "Charlie Kirk Answers The Question \"Why Do You Have To Be So Offensive?\" | Why Only Facts Matter          88\n",
      "Rob Smith: \"Socialism Is About Rules For Thee, Not For Me\"                                               73\n",
      "Rep. Gaetz To TPUSA Activists: \"Every Job In America Is Essential\" TPUSA Student Action Summit 2020      71\n",
      "TUCKER SHREDS LEFT-WING “DIVERSITY” GAMES...                                                             60\n",
      "What Are Your New Year’s Resolutions? [Benny On The Block 34]                                            30\n",
      "Best Of Real America! [Real America Episode 31]                                                          18\n",
      "Dr. Jeff Myers: \"Nobody Is Neutral. Everyone Has A Worldview.\"                                            9\n",
      "Varsity Spirit: Only in America                                                                           3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for model in videos_to_comment_on:\n",
    "    print(videos_to_comment_on_2[model].video_title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        if len(videos_comments_and_responses_embedded[affil][str(ms)]) < 2:\n",
    "            affil_label = 'l' if affil == 'progressive' else 'r'\n",
    "            other_video = sample([x for x in list(videos_to_comment_on_2[(affil, ms)].video_title.unique()) if x != videos_comments_and_responses_embedded[affil][str(ms)][0]['video_title']], 1)[0]\n",
    "            other_video_description = list(cleaned.loc[cleaned.video_title == other_video, :].video_description.unique())[0]\n",
    "            message_list = format_question_for_chat_completion(other_video, other_video_description, affil)\n",
    "            response1 = oai_client.chat.completions.create(\n",
    "                model=affiliated_models[affil_label][ms],\n",
    "                messages=message_list,\n",
    "                n=125\n",
    "            )\n",
    "            temp1 = [choice.message.content for choice in response1.choices]\n",
    "            response2 = oai_client.chat.completions.create(\n",
    "                model=affiliated_models[affil_label][ms],\n",
    "                messages=message_list,\n",
    "                n=125\n",
    "            )\n",
    "            temp2 = [choice.message.content for choice in response2.choices]\n",
    "            videos_comments_and_responses_embedded[affil][str(ms)].append({\n",
    "                'video_title' : other_video,\n",
    "                'llm_comments' : [get_embedding(x) for x in temp1+temp2]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video_title', 'human_comments', 'llm_comments'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_comments_and_responses_embedded['progressive']['25'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_12228\\3143676611.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cs_part2_df = pd.concat([cs_part2_df, df_1])\n"
     ]
    }
   ],
   "source": [
    "def get_synthetic_group(group):\n",
    "    return group[group['similarity_type'] == 'synthetic']\n",
    "\n",
    "cs_part2_df = pd.DataFrame(columns = ['affiliation',\n",
    "                                      'size',\n",
    "                                      'trial_id',\n",
    "                                      'similarity',\n",
    "                                      'similarity_type'\n",
    "                                      ])\n",
    "for affil in affils:\n",
    "    for ms in model_sizes:\n",
    "        data = {\n",
    "                'trial_id' : [],\n",
    "                'similarity' : [],\n",
    "                'similarity_type' : []   \n",
    "            }\n",
    "        \n",
    "        #Part 1 - get comparison for same video.\n",
    "        df_1 = cs_part1_df.groupby(['affiliation', 'size', 'similarity_type']).apply(get_synthetic_group).reset_index(drop=True)\n",
    "        df_1 = df_1[df_1['size'] == ms]\n",
    "        df_1 = df_1[df_1.affiliation == affil]\n",
    "        df_1['similarity_type'] = 'same_video'\n",
    "        cs_part2_df = pd.concat([cs_part2_df, df_1])\n",
    "        \n",
    "        for trial_id in range(1, 1001):\n",
    "            #Part 2 - get comparison for different video from same channel.\n",
    "            data = {\n",
    "                'trial_id' : [],\n",
    "                'similarity' : [],\n",
    "                'similarity_type' : []   \n",
    "            }\n",
    "            selected_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            while not 'human_comments' in videos_comments_and_responses_embedded[affil][str(ms)][selected_video].keys():\n",
    "                selected_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            comparison_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            while comparison_video == selected_video:\n",
    "                comparison_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            human_comment_for_comparison = sample(videos_comments_and_responses_embedded[affil][str(ms)][selected_video]['human_comments'], k=1)\n",
    "            llm_comment_list = sample(videos_comments_and_responses_embedded[affil][str(ms)][comparison_video]['llm_comments'], 100)\n",
    "            data['trial_id'].append(trial_id)\n",
    "            data['similarity'].append(np.mean([cosine_similarity(human_comment_for_comparison, x) for x in llm_comment_list]))\n",
    "            data['similarity_type'].append('same_channel')    \n",
    "            df_2 = pd.DataFrame(data)\n",
    "            df_2['affiliation'] = affil\n",
    "            df_2['size'] = ms\n",
    "            \n",
    "            \n",
    "            #Part 3 - get comparison for different channel\n",
    "            data = {\n",
    "                'trial_id' : [],\n",
    "                'similarity' : [],\n",
    "                'similarity_type' : []   \n",
    "            }\n",
    "            selected_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            while not 'human_comments' in videos_comments_and_responses_embedded[affil][str(ms)][selected_video].keys():\n",
    "                selected_video = sample(range(len(videos_comments_and_responses_embedded[affil][str(ms)])), 1)[0]\n",
    "            human_comment_for_comparison = sample(videos_comments_and_responses_embedded[affil][str(ms)][selected_video]['human_comments'], k=1) \n",
    "            comparison_video = sample(range(len(videos_comments_and_responses_other_channels_embedded[affil][str(ms)])), 1)[0]  \n",
    "            llm_comment_list = sample(videos_comments_and_responses_other_channels_embedded[affil][str(ms)]['llm_comments'], 100)\n",
    "            data['trial_id'].append(trial_id)\n",
    "            data['similarity'].append(np.mean([cosine_similarity(human_comment_for_comparison, x) for x in llm_comment_list]))\n",
    "            data['similarity_type'].append('different_channel')    \n",
    "            df_3 = pd.DataFrame(data)\n",
    "            df_3['affiliation'] = affil\n",
    "            df_3['size'] = ms\n",
    "            \n",
    "            cs_part2_df=pd.concat([cs_part2_df, df_2, df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_part2_df.to_csv(\"../data/experiment_rerun/cosine_similarity_synthetic_type_comparisons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = cs_part2_df.groupby(['affiliation', 'size', 'similarity_type']).agg({'similarity':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "pivoted_df = temp.pivot_table(\n",
    "    index=['size', 'similarity_type'],  # size and similarity_type as the new index\n",
    "    columns='affiliation',              # affiliation as the new columns\n",
    "    values='similarity'                 # similarity as the values to fill the DataFrame\n",
    ").swaplevel(0, 1, axis=0)               # swap levels to make similarity_type the outer index\n",
    "\n",
    "custom_order = ['same_video', 'same_channel', 'different_channel']\n",
    "\n",
    "# Reindex the DataFrame according to the custom order\n",
    "# You would do this on the level of the index where 'similarity_type' is\n",
    "pivoted_df = pivoted_df.reindex(custom_order, level='similarity_type')\n",
    "\n",
    "# Sort the index to have a nice hierarchical order\n",
    "#pivoted_df = pivoted_df.sort_index(level=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation</th>\n",
       "      <th>conservative</th>\n",
       "      <th>progressive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity_type</th>\n",
       "      <th>size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">same_video</th>\n",
       "      <th>25</th>\n",
       "      <td>0.183933</td>\n",
       "      <td>0.202204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.184084</td>\n",
       "      <td>0.198694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.175512</td>\n",
       "      <td>0.197850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.173395</td>\n",
       "      <td>0.195701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">same_channel</th>\n",
       "      <th>25</th>\n",
       "      <td>0.157763</td>\n",
       "      <td>0.180349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.161564</td>\n",
       "      <td>0.191112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.135790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.161635</td>\n",
       "      <td>0.139951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">different_channel</th>\n",
       "      <th>25</th>\n",
       "      <td>0.166106</td>\n",
       "      <td>0.163773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.174330</td>\n",
       "      <td>0.144167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.161526</td>\n",
       "      <td>0.167137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.170484</td>\n",
       "      <td>0.137028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "affiliation             conservative  progressive\n",
       "similarity_type   size                           \n",
       "same_video        25        0.183933     0.202204\n",
       "                  125       0.184084     0.198694\n",
       "                  625       0.175512     0.197850\n",
       "                  3125      0.173395     0.195701\n",
       "same_channel      25        0.157763     0.180349\n",
       "                  125       0.161564     0.191112\n",
       "                  625       0.165217     0.135790\n",
       "                  3125      0.161635     0.139951\n",
       "different_channel 25        0.166106     0.163773\n",
       "                  125       0.174330     0.144167\n",
       "                  625       0.161526     0.167137\n",
       "                  3125      0.170484     0.137028"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
