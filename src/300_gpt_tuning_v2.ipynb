{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300_gpt_tuning_v2\n",
    "\n",
    "> In this notebook we will fine tune multiple GPT 3.5 models with what we learned from the first set and with progressive and conservative models seperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import json\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable apis with keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/danie/OneDrive/Desktop/buffed_perspective_api_key_.txt\") as f:\n",
    "    papi_key1 = f.readline()\n",
    "    \n",
    "with open(\"C:/Users/danie/OneDrive/Desktop/perspective_api_key.txt\") as f:\n",
    "    papi_key2 = f.readline()\n",
    "    \n",
    "\n",
    "perspective_client0 = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=papi_key1,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "perspective_client1 = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=papi_key2,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"C:/Users/danie/OneDrive/Desktop/openai_youtube_api_key.txt\") as f:\n",
    "    api_key = f.readline()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifier \n",
    "def get_toxicity_score(comment, client):\n",
    "    analyze_request = {\n",
    "        'comment': { 'text': comment },\n",
    "        'languages': [\"en\"],\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = eval(client).comments().analyze(body=analyze_request).execute()\n",
    "        return float(response['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "    except HttpError as e:\n",
    "        return [e.resp.status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_toxicity_score_wrapper(inp):\n",
    "    if type(inp) == type({}):\n",
    "        content = inp\n",
    "    elif type(inp) == type([]):\n",
    "        if type(inp[0]) == type({}):\n",
    "            content = [l['messages'][2]['content'] for l in inp]\n",
    "    else:\n",
    "        return \"Invalid input type!\"\n",
    "\n",
    "    print(\"Beginning Toxcity Assessment:\")\n",
    "    toxicity_vals = []\n",
    "    switch_count = 0\n",
    "    total_lines = len(content)\n",
    "    current_client = f\"perspective_client{switch_count}\"\n",
    "    for i, l in enumerate(content):\n",
    "        s = get_toxicity_score(l, current_client)\n",
    "        if s == [429]:\n",
    "            print(\"Switched Perspective Clients!\")\n",
    "            switch_count +=1 \n",
    "            current_client = f\"perspective_client{switch_count % 2}\"\n",
    "            s = get_toxicity_score(l, current_client)\n",
    "        if type(s) != type(0.0):\n",
    "            return f\"Error! S = {s}\"\n",
    "        else:\n",
    "            toxicity_vals.append(s)\n",
    "        time.sleep(1)\n",
    "        percentage_complete = (i+1)/total_lines * 100\n",
    "        if percentage_complete % 1 == 0:\n",
    "            print(f\"{percentage_complete}% Processed.\")\n",
    "\n",
    "    return toxicity_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comment_for_finetuning(row):\n",
    "    affil = row.affiliation\n",
    "    affil = 'conservative' if affil == 'R' else 'progressive'\n",
    "    title = row.video_title\n",
    "    desc = row.video_description\n",
    "    comment = row.comment_text\n",
    "    formatted = {\"messages\": [{\"role\": \"system\", \"content\": f\"You are a {affil} American political commentator.\"},\n",
    "                              {\"role\": \"user\", \"content\": f\"You just watched the youtube video '{title}' with the description '{desc}'.\\n\\nGive your opinion on the subject matter.\"},\n",
    "                              {\"role\": \"assistant\", \"content\": comment}]}\n",
    "    return json.dumps(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['channel_id', 'channel_title', 'affiliation', 'video_id', 'video_title',\n",
       "       'video_description', 'comment_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv(\"../data/cleaned/channel_subset_with_comments.csv\", index_col='comment_id')\n",
    "comments_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33762 entries, 0 to 33761\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   channel_id         33762 non-null  object\n",
      " 1   channel_title      33762 non-null  object\n",
      " 2   affiliation        33762 non-null  object\n",
      " 3   video_id           33762 non-null  object\n",
      " 4   video_title        33762 non-null  object\n",
      " 5   video_description  33762 non-null  object\n",
      " 6   comment_text       33762 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42706 entries, 0 to 42705\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   channel_id         42706 non-null  object\n",
      " 1   channel_title      42706 non-null  object\n",
      " 2   affiliation        42706 non-null  object\n",
      " 3   video_id           42706 non-null  object\n",
      " 4   video_title        42706 non-null  object\n",
      " 5   video_description  42706 non-null  object\n",
      " 6   comment_text       42706 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Right data will be from Turning Point USA\n",
    "right_comments = comments_df.loc[comments_df.affiliation == 'R']\n",
    "right_comments.reset_index(drop=True, inplace=True)\n",
    "# Left comments will be from The Young Turks \n",
    "left_comments =comments_df.loc[comments_df.affiliation == \"L\"]\n",
    "left_comments.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(right_comments.info())\n",
    "print(left_comments.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 125, 625, 3125]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sizes = np.logspace(2, 5, 4, base=5).astype(int).tolist()\n",
    "model_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets begin by creating a function which can iteratively reduce a set of comments down to size of the largest desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flagged_lines(msg):\n",
    "    idx = msg.index('Flagged lines: ') + len('Flagged lines: ')\n",
    "    temp = msg[idx:].split(', ')\n",
    "    try:\n",
    "        if temp[-1].index(' ') > -1:\n",
    "            temp[-1] = temp[-1][:temp[-1].index(' ')]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return set([int(x) for x in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_file(df, affil, final=False):\n",
    "    size = df.shape[0]\n",
    "    if final:\n",
    "        if affil == 'l':\n",
    "            path = f'../data/final_ft_datasets/progressive_size_{size}_train.jsonl'\n",
    "        else:\n",
    "            path = f'../data/final_ft_datasets/conservative_size_{size}_train.jsonl'\n",
    "    else:\n",
    "        if affil == 'l':\n",
    "            path = f'../data/cleaned/progressive_size_{size}_train.jsonl'\n",
    "        else:\n",
    "            path = f'../data/cleaned/conservative_size_{size}_train.jsonl'\n",
    "        \n",
    "    ft_train = open(path, mode='w')\n",
    "\n",
    "    for i in range(size):\n",
    "        row_dic = format_comment_for_finetuning(df.iloc[i,:])\n",
    "        ft_train.write(row_dic)\n",
    "        ft_train.write('\\n')\n",
    "    \n",
    "    print(f\"Successfully wrote size {affil} {size} training file.\")\n",
    "    \n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_finetuning_set(df, backup_df, desired_size, affil, flagged_lines=[], reduce=False, skip = False, edge_case=False):\n",
    "    if not skip:\n",
    "        ftj_obj = {\"status\":\"skipped\"}\n",
    "        if not flagged_lines and not reduce:\n",
    "            if affil == 'l':\n",
    "                affil_label = \"progressive\"\n",
    "            else:\n",
    "                affil_label = \"conservative\"\n",
    "                \n",
    "            #Create initial File  \n",
    "            size = df.shape[0]\n",
    "            print(f\"Reducing dataframe with {size} examples.\")\n",
    "            path = create_training_file(df, affil)\n",
    "            upload_response = openai.File.create(\n",
    "                file=open(path, \"rb\"),\n",
    "                purpose='fine-tune',\n",
    "                user_provided_filename=f'{affil_label}_size_{size}_training_examples'\n",
    "            )\n",
    "        \n",
    "            print(\"Creating fine-tuning job.\")\n",
    "            #Attempt to fine_tune on entire list \n",
    "            file_id = upload_response['id']\n",
    "            ftj_response = openai.FineTuningJob.create(\n",
    "                training_file=file_id,\n",
    "                model=\"gpt-3.5-turbo\"\n",
    "            )\n",
    "\n",
    "            base_cases = [\"failed\", \"succeeded\", \"cancelled\"]\n",
    "            loop_count = 1\n",
    "            while openai.FineTuningJob.retrieve(ftj_response['id'])['status'] not in base_cases:\n",
    "                time.sleep(5)\n",
    "                if loop_count % 60 == 0:\n",
    "                    print(f\"Waiting for finetuning job!\\nTime: {loop_count*5//60} minutes\")\n",
    "                loop_count+=1\n",
    "            \n",
    "            ftj_obj = openai.FineTuningJob.retrieve(ftj_response['id'])  \n",
    "            \n",
    "        if flagged_lines or ftj_obj['status'] == 'failed':\n",
    "            print(\"Finetuning job failed. Recursing.\")\n",
    "            if not flagged_lines:\n",
    "                msg = ftj_obj['error']['message']\n",
    "                flagged_lines = get_flagged_lines(msg)\n",
    "            reduced_df = df.drop(flagged_lines)\n",
    "            if reduced_df.shape[0] < desired_size:\n",
    "                added_count = desired_size-reduced_df.shape[0]\n",
    "                print(f\"Added {added_count} examples from backup dataframe.\")\n",
    "                reduced_df = pd.concat([reduced_df, backup_df.sample(n=added_count)])\n",
    "                edge_case = True\n",
    "            else:\n",
    "                edge_case = False\n",
    "            reduced_df.reset_index(drop=True, inplace=True)\n",
    "            return reduce_finetuning_set(reduced_df, backup_df, desired_size, affil, edge_case=edge_case)\n",
    "        else:\n",
    "            if edge_case:\n",
    "                return ftj_obj, df\n",
    "            else:\n",
    "                print(\"Finetuning job succeeded.\")\n",
    "                if size > desired_size:\n",
    "                    print(f\"Succeeded with dataframe of {size}. Reducing to {desired_size}.\")\n",
    "                    reduced_df = df.sample(n=desired_size)\n",
    "                    final_path = create_training_file(reduced_df,affil,final=True)\n",
    "                else:\n",
    "                    final_path = create_training_file(df,affil,final=True)    \n",
    "                    \n",
    "                final_upload_response = openai.File.create(\n",
    "                    file=open(final_path, \"rb\"),\n",
    "                    purpose='fine-tune',\n",
    "                    user_provided_filename=f'{affil_label}_size_{desired_size}_training_examples'\n",
    "                )\n",
    "            \n",
    "                final_file_id = final_upload_response['id']\n",
    "                final_ftj_response = openai.FineTuningJob.create(\n",
    "                    training_file=final_file_id,\n",
    "                    model=\"gpt-3.5-turbo\"\n",
    "                )\n",
    "                \n",
    "                print(f\"Final {affil_label} fine-tuning file of size {desired_size} created.\")\n",
    "                return final_ftj_response, reduced_df\n",
    "    else:\n",
    "        size = df.shape[0]\n",
    "        if affil == 'l':\n",
    "            affil_label = \"progressive\"\n",
    "        else:\n",
    "            affil_label = \"conservative\"\n",
    "        if size > desired_size:\n",
    "            print(f\"Reducing dataframe of {size} to {desired_size}.\")\n",
    "            reduced_df = df.sample(n=desired_size)\n",
    "            final_path = create_training_file(reduced_df,affil,final=True)\n",
    "        else:\n",
    "            final_path = create_training_file(df,affil,final=True)    \n",
    "            \n",
    "        final_upload_response = openai.File.create(\n",
    "            file=open(final_path, \"rb\"),\n",
    "            purpose='fine-tune',\n",
    "            user_provided_filename=f'{affil_label}_size_{desired_size}_training_examples'\n",
    "        )\n",
    "    \n",
    "        final_file_id = final_upload_response['id']\n",
    "        final_ftj_response = openai.FineTuningJob.create(\n",
    "            training_file=final_file_id,\n",
    "            model=\"gpt-3.5-turbo\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Final {affil_label} fine-tuning file of size {desired_size} created.\")\n",
    "        return final_ftj_response, reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 213, 1827, 15625]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sizes = np.logspace(2, 6, 4, base=5).astype(int).tolist()\n",
    "model_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOOOOOOOOOOOOOPS \n",
    "\n",
    "model_sizes = np.logspace(2, 6, 5, base=5).astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_affliated_models(comments, affil, model_sizes):\n",
    "    largest_model = model_sizes[-1]\n",
    "    seed = 123\n",
    "    old_df = comments\n",
    "    new_df = comments.sample(n=int(largest_model+(.25*largest_model)), random_state=seed).reset_index(drop=True)\n",
    "    models = {}\n",
    "    skip = False\n",
    "    for i in range(len(model_sizes)-1, -1, -1):\n",
    "        model_size= model_sizes[i]\n",
    "        response, temp_df = reduce_finetuning_set(new_df, old_df, model_size, affil, skip=skip)\n",
    "        while openai.FineTuningJob.retrieve(response['id'])['status'] != 'succeeded':\n",
    "                time.sleep(5)\n",
    "        models[model_size] = response['id']\n",
    "        old_df = new_df.reset_index(drop=True)\n",
    "        new_df = temp_df.reset_index(drop=True)\n",
    "        skip = True\n",
    "    return models\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now let's create our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dataframe with 156 examples.\n",
      "Successfully wrote size r 156 training file.\n",
      "Creating fine-tuning job.\n",
      "Waiting for finetuning job!\n",
      "Time: 5 minutes\n",
      "Waiting for finetuning job!\n",
      "Time: 10 minutes\n",
      "Waiting for finetuning job!\n",
      "Time: 15 minutes\n",
      "Finetuning job succeeded.\n",
      "Succeeded with dataframe of 156. Reducing to 125.\n",
      "Successfully wrote size r 125 training file.\n",
      "Final conservative fine-tuning file of size 125 created.\n",
      "Reducing dataframe of 125 to 25.\n",
      "Successfully wrote size r 25 training file.\n",
      "Final conservative fine-tuning file of size 25 created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{125: 'ftjob-dUQD3xBBWUQ33mA1AcTRUxSr', 25: 'ftjob-q5UHn926xz9g8wfdCPjFuK7U'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_affliated_models(right_comments, 'r', model_sizes[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
